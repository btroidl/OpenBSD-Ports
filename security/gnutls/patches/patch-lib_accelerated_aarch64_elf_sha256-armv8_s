Index: lib/accelerated/aarch64/elf/sha256-armv8.s
--- lib/accelerated/aarch64/elf/sha256-armv8.s.orig
+++ lib/accelerated/aarch64/elf/sha256-armv8.s
@@ -58,11 +58,8 @@ sha256_block_data_order:
 
 
 
- ldr x16,.L_gnutls_arm_cpuid_s
-
- adr x17,.L_gnutls_arm_cpuid_s
- add x16,x16,x17
- ldr w16,[x16]
+ adrp x16,_gnutls_arm_cpuid_s
+ ldr w16,[x16,#:lo12:_gnutls_arm_cpuid_s]
  tst w16,#(1<<4)
  b.ne .Lv8_entry
  tst w16,#(1<<0)
@@ -84,7 +81,8 @@ sha256_block_data_order:
  ldp w24,w25,[x0,#4*4]
  add x2,x1,x2,lsl#6
  ldp w26,w27,[x0,#6*4]
- adr x30,.LK256
+ adrp x30,.LK256
+ add x30,x30,:lo12:.LK256
  stp x0,x2,[x29,#96]
 
 .Loop:
@@ -1032,6 +1030,7 @@ sha256_block_data_order:
  ret
 .size sha256_block_data_order,.-sha256_block_data_order
 
+.rodata
 .align 6
 .type .LK256,%object
 .LK256:
@@ -1065,6 +1064,7 @@ sha256_block_data_order:
 .byte 83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,65,82,77,118,56,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 .align 2
 .align 2
+.previous
 
 .type sha256_block_armv8,%function
 .align 6
@@ -1074,7 +1074,8 @@ sha256_block_armv8:
  add x29,sp,#0
 
  ld1 {v0.4s,v1.4s},[x0]
- adr x3,.LK256
+ adrp x3,.LK256
+ add x3,x3,:lo12:.LK256
 
 .Loop_hw:
  ld1 {v4.16b,v5.16b,v6.16b,v7.16b},[x1],#64
@@ -1216,7 +1217,8 @@ sha256_block_neon:
  mov x29, sp
  sub sp,sp,#16*4
 
- adr x16,.LK256
+ adrp x16,.LK256
+ add x16,x16,:lo12:.LK256
  add x2,x1,x2,lsl#6
 
  ld1 {v0.16b},[x1], #16
